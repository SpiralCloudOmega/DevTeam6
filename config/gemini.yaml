# Google Gemini CLI Configuration
# Comprehensive settings for Gemini integration

gemini:
  api:
    key: ${GEMINI_API_KEY}
    endpoint: https://generativelanguage.googleapis.com/v1
    version: v1
  
  models:
    primary: gemini-2.0-flash-exp
    fallback: gemini-pro
    ultra: gemini-ultra
    
  # Model-specific settings
  model_config:
    gemini-2.0-flash-exp:
      max_tokens: 8192
      temperature: 0.7
      top_p: 0.95
      top_k: 40
      
    gemini-pro:
      max_tokens: 32768
      temperature: 0.8
      top_p: 0.95
      top_k: 40
      
  # Token optimization
  optimization:
    use_skill_cache: true
    skill_cache_path: ./skills/cache
    max_context_tokens: 100000
    enable_procedural_memory: true
    compress_prompts: true
    deduplicate_context: true
    
  # Safety settings
  safety:
    harm_block_threshold: BLOCK_MEDIUM_AND_ABOVE
    enable_filters: true
    categories:
      - HARM_CATEGORY_HARASSMENT
      - HARM_CATEGORY_HATE_SPEECH
      - HARM_CATEGORY_SEXUALLY_EXPLICIT
      - HARM_CATEGORY_DANGEROUS_CONTENT
      
  # Performance
  performance:
    stream_responses: true
    timeout: 300
    retry_attempts: 3
    rate_limit_rpm: 60
    batch_requests: true
    max_batch_size: 10
    
  # Integration points
  integrations:
    jules_agent: true
    conductor: true
    spec_kit: true
    rag_pipeline: true
    mcp_servers: true
    
  # Logging
  logging:
    level: INFO
    file: ./logs/gemini.log
    debug_mode: false
    log_requests: true
    log_responses: false
    
# Claude alternative (interchangeable)
claude:
  enabled: false
  api:
    key: ${CLAUDE_API_KEY}
    endpoint: https://api.anthropic.com/v1
    version: 2023-06-01
    
  models:
    primary: claude-3-opus-20240229
    sonnet: claude-3-sonnet-20240229
    haiku: claude-3-haiku-20240307
    
  model_config:
    claude-3-opus-20240229:
      max_tokens: 4096
      temperature: 0.7
      top_p: 0.95
